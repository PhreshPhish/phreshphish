{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591e2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute confidence intervals for evaluation metrics using stratified bootstrapping.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "def precision_at_recall(y_true: np.ndarray, y_score: np.ndarray, target_recall: float = 0.9) -> float:\n",
    "    \"\"\"\n",
    "    Compute precision at a specific recall threshold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground truth binary labels.\n",
    "    y_score : array-like\n",
    "        Predicted probability scores.\n",
    "    target_recall : float\n",
    "        Target recall value (default 0.9).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Precision at the specified recall level.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    \n",
    "    # Find the precision at the target recall\n",
    "    # precision_recall_curve returns values in decreasing recall order\n",
    "    # Find indices where recall >= target_recall\n",
    "    valid_indices = np.where(recall >= target_recall)[0]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Return the maximum precision at recall >= target_recall\n",
    "    return precision[valid_indices].max()\n",
    "\n",
    "\n",
    "def stratified_bootstrap_sample(df: pd.DataFrame, random_state: np.random.RandomState) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a stratified bootstrap sample, sampling each class separately.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with 'GT' and 'score' columns.\n",
    "    random_state : np.random.RandomState\n",
    "        Random state for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Bootstrapped sample with same class distribution.\n",
    "    \"\"\"\n",
    "    class_0 = df[df['GT'] == 0]\n",
    "    class_1 = df[df['GT'] == 1]\n",
    "    \n",
    "    # Sample with replacement from each class\n",
    "    bootstrap_0 = class_0.sample(n=len(class_0), replace=True, random_state=random_state)\n",
    "    bootstrap_1 = class_1.sample(n=len(class_1), replace=True, random_state=random_state)\n",
    "    \n",
    "    return pd.concat([bootstrap_0, bootstrap_1], ignore_index=True)\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute average precision and precision at recall=0.9.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground truth binary labels.\n",
    "    y_score : array-like\n",
    "        Predicted probability scores.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (average_precision, precision_at_recall_0.9)\n",
    "    \"\"\"\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    p_at_r = precision_at_recall(y_true, y_score, target_recall=0.9)\n",
    "    \n",
    "    return ap, p_at_r\n",
    "\n",
    "\n",
    "def compute_confidence_intervals(\n",
    "    df: pd.DataFrame,\n",
    "    n_bootstrap: int = 10000,\n",
    "    confidence_level: float = 0.95,\n",
    "    random_seed: int = 42\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute confidence intervals for evaluation metrics using stratified bootstrapping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "        - 'GT': Ground truth labels (0 or 1)\n",
    "        - 'score': Predicted probability scores\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap iterations (default 10000).\n",
    "    confidence_level : float\n",
    "        Confidence level for intervals (default 0.95 for 95% CI).\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing mean, lower CI, and upper CI for each metric.\n",
    "        {\n",
    "            'average_precision': {'mean': float, 'lower_ci': float, 'upper_ci': float},\n",
    "            'precision_at_recall_0.9': {'mean': float, 'lower_ci': float, 'upper_ci': float}\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if 'GT' not in df.columns or 'score' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'GT' and 'score' columns\")\n",
    "    \n",
    "    if not set(df['GT'].unique()).issubset({0, 1}):\n",
    "        raise ValueError(\"GT column must contain only 0 and 1 values\")\n",
    "    \n",
    "    # Initialize random state\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    \n",
    "    # Store bootstrap results\n",
    "    ap_scores = np.zeros(n_bootstrap)\n",
    "    p_at_r_scores = np.zeros(n_bootstrap)\n",
    "    \n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_bootstrap):\n",
    "        # Create stratified bootstrap sample\n",
    "        bootstrap_df = stratified_bootstrap_sample(df, rng)\n",
    "        \n",
    "        y_true = bootstrap_df['GT'].values\n",
    "        y_score = bootstrap_df['score'].values\n",
    "        \n",
    "        # Compute metrics\n",
    "        ap, p_at_r = compute_metrics(y_true, y_score)\n",
    "        \n",
    "        ap_scores[i] = ap\n",
    "        p_at_r_scores[i] = p_at_r\n",
    "    \n",
    "    # Compute confidence intervals using percentile method\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    results = {\n",
    "        'average_precision': {\n",
    "            'mean': np.mean(ap_scores),\n",
    "            'lower_ci': np.percentile(ap_scores, lower_percentile),\n",
    "            'upper_ci': np.percentile(ap_scores, upper_percentile)\n",
    "        },\n",
    "        'precision_at_recall_0.9': {\n",
    "            'mean': np.mean(p_at_r_scores),\n",
    "            'lower_ci': np.percentile(p_at_r_scores, lower_percentile),\n",
    "            'upper_ci': np.percentile(p_at_r_scores, upper_percentile)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results: Dict[str, Dict[str, float]]) -> None:\n",
    "    \"\"\"\n",
    "    Pretty print the confidence interval results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Results dictionary from compute_confidence_intervals.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Evaluation Metrics with 95% Confidence Intervals\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for metric_name, values in results.items():\n",
    "        print(f\"\\n{metric_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Mean:     {values['mean']:.4f}\")\n",
    "        print(f\"  95% CI:   [{values['lower_ci']:.4f}, {values['upper_ci']:.4f}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a0aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: (1000, 2)\n",
      "Class distribution: {0: 712, 1: 288}\n",
      "\n",
      "============================================================\n",
      "Evaluation Metrics with 95% Confidence Intervals\n",
      "============================================================\n",
      "\n",
      "Average Precision:\n",
      "  Mean:     0.9107\n",
      "  95% CI:   [0.8882, 0.9309]\n",
      "\n",
      "Precision At Recall 0.9:\n",
      "  Mean:     0.6302\n",
      "  95% CI:   [0.5874, 0.6779]\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create sample data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic ground truth and scores\n",
    "gt = np.random.binomial(1, 0.3, n_samples)  # 30% positive class\n",
    "\n",
    "# Generate scores that are somewhat correlated with ground truth\n",
    "scores = np.clip(\n",
    "    gt * np.random.uniform(0.4, 1.0, n_samples) + \n",
    "    (1 - gt) * np.random.uniform(0.0, 0.6, n_samples),\n",
    "    0, 1\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'GT': gt,\n",
    "    'score': scores\n",
    "})\n",
    "\n",
    "print(f\"Sample data shape: {df.shape}\")\n",
    "print(f\"Class distribution: {df['GT'].value_counts().to_dict()}\")\n",
    "print()\n",
    "\n",
    "# Compute confidence intervals\n",
    "results = compute_confidence_intervals(df, n_bootstrap=10000, random_seed=42)\n",
    "\n",
    "# Print results\n",
    "print_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
