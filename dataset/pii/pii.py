"""Constructs a list of SHA256s that contain PII in the datset."""

import polars as pl
import json
from tqdm import tqdm
from urllib.parse import urlparse, unquote

from .data import PhreshPhishDataset
from .models import load

from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch


def load_ner_model():
    """Loads the BERT NER model."""

    print("Loading NER model...", end="")
    tokenizer = AutoTokenizer.from_pretrained(
        "dslim/bert-base-NER",
    )
    model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER").to(
        "cuda"
    )
    model.eval()
    print("done")
    return tokenizer, model


def is_probable_name(name: str) -> bool:
    """Check if the extracted name looks like a real person name."""
    parts = name.strip().split()
    return (
        2 <= len(parts) <= 4
        and all(p[0].isupper() for p in parts if p.isalpha())
        and not any(
            p.lower() in {"hair", "shop", "store", "gmbh", "restaurant"} for p in parts
        )
    )


def extract_name_from_tokens(
    text: str, pred, prob_row, offsets, id2label, min_score: float
) -> str | None:
    """Use NER predictions and offsets to extract a full name from one candidate string."""
    current_tokens = []

    for idx, label_id in enumerate(pred):
        label = id2label[label_id.item()]
        confidence = prob_row[idx][label_id].item()
        start, end = offsets[idx]

        if label.startswith("B-PER") and confidence >= min_score:
            current_tokens = [text[start:end]]
        elif label.startswith("I-PER") and confidence >= min_score and current_tokens:
            current_tokens.append(text[start:end])
        elif current_tokens:
            break  # end of entity

    full_name = " ".join(current_tokens).strip()
    return full_name if current_tokens and is_probable_name(full_name) else None


def deduplicate_results(results: list[dict]) -> list[dict]:
    """Remove duplicates based on (sha256, name)."""
    seen = set()
    deduped = []

    for entry in results:
        key = (entry["sha256"], entry["name"])
        if key not in seen:
            seen.add(key)
            deduped.append(entry)

    return deduped


def run_inference(
    dataset, tokenizer, model, min_score=0.99, batch_size=32
) -> list[dict]:
    """Run batched NER and return deduplicated person names extracted from candidates."""
    device = model.device
    rows = dataset.get_hf_dataset()
    dataloader = dataset.get_dataloader(batch_size=batch_size)
    id2label = model.config.id2label

    results = []
    row_idx = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Running NER"):
            encodings = tokenizer(
                batch,
                return_offsets_mapping=True,
                return_tensors="pt",
                padding=True,
                truncation=True,
            ).to(device)

            offset_mappings = encodings.pop("offset_mapping").cpu().tolist()
            outputs = model(**encodings)

            preds = torch.argmax(outputs.logits, dim=-1)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)

            for b_idx in range(len(batch)):
                text = batch[b_idx]
                pred = preds[b_idx]
                prob_row = probs[b_idx]
                offsets = offset_mappings[b_idx]
                row = rows[row_idx + b_idx]

                name = extract_name_from_tokens(
                    text, pred, prob_row, offsets, id2label, min_score
                )
                if name:
                    results.append(
                        {"sha256": row["sha256"], "url": row["url"], "name": name}
                    )

            row_idx += len(batch)

    return deduplicate_results(results)


def find_pii(
    path: str,
    output_path: str = "pii.json",
    batch_size: int = 32,
    min_score: float = 0.99,
):
    tokenizer, model = load_ner_model()
    dataset = PhreshPhishDataset(path)
    results = run_inference(
        dataset, tokenizer, model, min_score=min_score, batch_size=batch_size
    )

    with open(output_path, "w") as f:
        for entry in results:
            f.write(json.dumps(entry) + "\n")

    print(f"Saved {len(results)} entries to {output_path}")


def remove_fps(path: str, output_path: str):
    """Removes false positives from the PII file generated by find_pii.
    This is done by invoking an LLM to check if the name is likely to be
    a name belonging to a customer."""

    llm = load()

    with open(path, "r") as f:
        lines = f.readlines()

    lines = [json.loads(line) for line in lines]
    with open(output_path, "w") as fp:
        for entry in tqdm(lines, desc="Running LLM"):
            name = entry["name"]
            url = entry["url"]
            question = f"<url>{url}</url>" f"<name>{name}</name>"

            answer = llm.answer(question)
            entry = {
                "sha256": entry["sha256"],
                "url": entry["url"],
                "name": entry["name"],
                "pii": answer,
            }

            fp.write(json.dumps(entry) + "\n")
            fp.flush()


if __name__ == "__main__":
    import fire

    fire.Fire({"find_pii": find_pii, "remove_fps": remove_fps})
